{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv8\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponível\n",
      "Ultralytics: 8.0.106\n",
      "Torch: 2.1.1+cu121\n",
      "supervision: 0.16.0\n"
     ]
    }
   ],
   "source": [
    "print('CUDA disponível' if torch.cuda.is_available() else 'CUDA indisponível')\n",
    "print(f'Ultralytics: {ultralytics.__version__}')\n",
    "print(f'Torch: {torch.__version__}')\n",
    "print(f'supervision: {sv.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Carregando o modelo pré-treinado\n",
    "model = YOLO('yolov8n.pt')\n",
    "model.fuse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "print(model.model.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando apenas as classes relevantes pro problema\n",
    "classes = [2, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoInfo(width=1280, height=720, fps=25, total_frames=750)\n"
     ]
    }
   ],
   "source": [
    "# Carrega o vídeo\n",
    "VIDEO = './dataset/road_video001.mp4'\n",
    "OUTPUT = 'output_video.mp4'\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(VIDEO)\n",
    "print(video_info)\n",
    "\n",
    "# Linha para contagem de carros\n",
    "START = sv.Point(0, 2*video_info.height//3)\n",
    "END = sv.Point(video_info.width, 2*video_info.height//3)\n",
    "\n",
    "carros = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 14:55:32.974102: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-03 14:55:32.974287: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-03 14:55:33.041154: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[ WARN:0@32.044] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@32.044] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "1/1: 0... Failed to open 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mwith\u001b[39;00m sv\u001b[39m.\u001b[39mVideoSink(target_path\u001b[39m=\u001b[39mOUTPUT, video_info\u001b[39m=\u001b[39mvideo_info) \u001b[39mas\u001b[39;00m sink:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mfor\u001b[39;00m frame, index \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sv\u001b[39m.\u001b[39mget_video_frames_generator(source_path\u001b[39m=\u001b[39mVIDEO)):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m         annotated_frame \u001b[39m=\u001b[39m callback(frame, index)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         sink\u001b[39m.\u001b[39mwrite_frame(frame\u001b[39m=\u001b[39mannotated_frame)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Mostra o vídeo\u001b[39;00m\n",
      "\u001b[1;32m/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcallback\u001b[39m(frame: np\u001b[39m.\u001b[39mndarray, index:\u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     results \u001b[39m=\u001b[39m model(frame, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     detections \u001b[39m=\u001b[39m sv\u001b[39m.\u001b[39mDetections\u001b[39m.\u001b[39mfrom_ultralytics(results)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucca-wsl/Visio2/visio-pesquisa/supervision2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     detections \u001b[39m=\u001b[39m detections[np\u001b[39m.\u001b[39misin(detections\u001b[39m.\u001b[39mclass_id, classes)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/model.py:111\u001b[0m, in \u001b[0;36mYOLO.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, source\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(source, stream, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/model.py:253\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs, overrides)\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/predictor.py:184\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model)\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/predictor.py:215\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetup_model(model)\n\u001b[1;32m    214\u001b[0m \u001b[39m# Setup source every time predict is called\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_source(source \u001b[39mif\u001b[39;49;00m source \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49msource)\n\u001b[1;32m    217\u001b[0m \u001b[39m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39msave \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39msave_txt:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/predictor.py:197\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgsz \u001b[39m=\u001b[39m check_imgsz(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mimgsz, stride\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstride, min_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# check image size\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmodel, \u001b[39m'\u001b[39m\u001b[39mtransforms\u001b[39m\u001b[39m'\u001b[39m, classify_transforms(\n\u001b[1;32m    196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgsz[\u001b[39m0\u001b[39m])) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mclassify\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m load_inference_source(source\u001b[39m=\u001b[39;49msource, imgsz\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimgsz, vid_stride\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mvid_stride)\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39msource_type\n\u001b[1;32m    199\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m  \u001b[39m# streams\u001b[39;00m\n\u001b[1;32m    200\u001b[0m                                           \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset) \u001b[39m>\u001b[39m \u001b[39m1000\u001b[39m \u001b[39mor\u001b[39;00m  \u001b[39m# images\u001b[39;00m\n\u001b[1;32m    201\u001b[0m                                           \u001b[39many\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m'\u001b[39m\u001b[39mvideo_flag\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39mFalse\u001b[39;00m]))):  \u001b[39m# videos\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/yolo/data/build.py:158\u001b[0m, in \u001b[0;36mload_inference_source\u001b[0;34m(source, imgsz, vid_stride)\u001b[0m\n\u001b[1;32m    156\u001b[0m     dataset \u001b[39m=\u001b[39m source\n\u001b[1;32m    157\u001b[0m \u001b[39melif\u001b[39;00m webcam:\n\u001b[0;32m--> 158\u001b[0m     dataset \u001b[39m=\u001b[39m LoadStreams(source, imgsz\u001b[39m=\u001b[39;49mimgsz, vid_stride\u001b[39m=\u001b[39;49mvid_stride)\n\u001b[1;32m    159\u001b[0m \u001b[39melif\u001b[39;00m screenshot:\n\u001b[1;32m    160\u001b[0m     dataset \u001b[39m=\u001b[39m LoadScreenshots(source, imgsz\u001b[39m=\u001b[39mimgsz)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/yolo/data/dataloaders/stream_loaders.py:57\u001b[0m, in \u001b[0;36mLoadStreams.__init__\u001b[0;34m(self, sources, imgsz, vid_stride)\u001b[0m\n\u001b[1;32m     55\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(s)\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m cap\u001b[39m.\u001b[39misOpened():\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mst\u001b[39m}\u001b[39;00m\u001b[39mFailed to open \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m w \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(cap\u001b[39m.\u001b[39mget(cv2\u001b[39m.\u001b[39mCAP_PROP_FRAME_WIDTH))\n\u001b[1;32m     59\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(cap\u001b[39m.\u001b[39mget(cv2\u001b[39m.\u001b[39mCAP_PROP_FRAME_HEIGHT))\n",
      "\u001b[0;31mConnectionError\u001b[0m: 1/1: 0... Failed to open 0"
     ]
    }
   ],
   "source": [
    "# Annotators\n",
    "line_zone = sv.LineZone(START, END)\n",
    "line_annotator = sv.LineZoneAnnotator(thickness=2, text_thickness=2, text_scale=1)\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(thickness=1, text_thickness=1, text_scale=0.5)\n",
    "\n",
    "byte_tracker = sv.ByteTrack(track_thresh=0.25, track_buffer=30, match_thresh=0.8, frame_rate=video_info.fps)\n",
    "\n",
    "def callback(frame: np.ndarray, index:int) -> np.ndarray:\n",
    "    results = model(frame, verbose=False)[0]\n",
    "        \n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    detections = detections[np.isin(detections.class_id, classes)]\n",
    "    detections = byte_tracker.update_with_detections(detections)\n",
    "    \n",
    "    labels = [\n",
    "        f'#{tracker_id} {model.model.names[class_id]} {confidence:.2f}'\n",
    "        for _, _, confidence, class_id, tracker_id in detections\n",
    "    ]\n",
    "    \n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=frame.copy(),\n",
    "        detections=detections,\n",
    "        labels=labels\n",
    "    )\n",
    "        \n",
    "    line_zone.trigger(detections)\n",
    "        \n",
    "    return line_annotator.annotate(annotated_frame, line_counter=line_zone)\n",
    "\n",
    "# Processar cafa frame, visualizar e montar o vídeo\n",
    "with sv.VideoSink(target_path=OUTPUT, video_info=video_info) as sink:\n",
    "    for frame, index in enumerate(sv.get_video_frames_generator(source_path=VIDEO)):\n",
    "        annotated_frame = callback(frame, index)\n",
    "        sink.write_frame(frame=annotated_frame)\n",
    "        \n",
    "        # Mostra o vídeo\n",
    "        cv2.imshow('frame', annotated_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Contagem dos carros\n",
    "carros = line_zone.in_count + line_zone.out_count\n",
    "print(f'Carros: {carros}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
